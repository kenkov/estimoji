#! /usr/bin/env python
# coding: utf-8


import emoji
import os


class Tokenizer:
    def tokenize(self, text):
            return text.split(" ")


def load_emoji_id():
    filename = os.path.join(os.path.abspath(os.path.dirname(__file__)),
                            "emoji_id.txt")
    emoji_id = dict()
    with open(filename) as f:
        for i, line in enumerate(f):
            emoji = line.strip("\n")
            emoji_id[emoji] = i
        return emoji_id


def extract_emoji(text):
    """æ–‡ã‚’é€£ç¶šã™ã‚‹å¥ç‚¹ã¨çµµæ–‡å­—ã®ç®‡æ‰€ã§åŒºåˆ‡ã‚‹ã€‚
    çµµæ–‡å­—ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆã¯ä»˜ä¸Žã•ã‚Œã¦ã„ã‚‹çµµæ–‡å­—ã‚’è¿”ã™

    ä¾‹:

        >>> text = "ã‚ã‚ŠãŒã¨ã†ã€‚ðŸ™ãã®ã†ã¡è¡Œã“ã†ã‹ðŸ˜Š"
        >>> split(text)
    """
    chars = []
    emojis = []
    sent_emoji = []

    for c in text:
        if c in {"ã€‚"}:
            pass
        elif c in emoji.UNICODE_EMOJI:
                emojis.append(c)
        else:
            if emojis and chars:
                sent_emoji.append(("".join(chars), set(emojis)))
                chars = []
                emojis = []
            chars.append(c)

    return dict(sent_emoji)